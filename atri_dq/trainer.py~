from deepLizard import *

batch_size = 256*4
gamma = 0.99
eps_start = 1
eps_end = 0.01
eps_decay = 0.01
target_update = 100
weight_update = 1
weight_increment = 250
weight_first_inc = 250
lr_step = 100
memory_size = 50000
lr = 0.007
num_episodes = 5000
loss_function = nn.HuberLoss()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using", device, "for computations")


#################em = CartPoleEnvManager(device,STATE_TYPE)
strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)
agent = Agent(strategy, em.num_actions_available(), device)
memory = ReplayMemory(memory_size)

policy_net = DQN(em.get_screen_height(), em.get_screen_width()).to(device)
target_net = DQN(em.get_screen_height(), em.get_screen_width()).to(device)
QV = QValues()
    
target_net.eval()
optimizer = optim.Adam(params = policy_net.parameters(), lr=lr)#,weight_decay = 0.1)
scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[1500,3000],gamma= 0.1)
#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience = 200, cooldown = 100)
episode_durations = []
step = 0
for episode in range(num_episodes):
    em.reset()
    state=em.get_state()

    for timestep in count():
        step += 1
        action = agent.select_action(state, policy_net, STATE_TYPE)
        reward = em.take_action(action)
        next_state = em.get_state()
        memory.push(Experience(state, action, next_state, reward))
        state = next_state

        if memory.can_provide_sample(batch_size) and step % weight_update == 0:
            experiences = memory.sample(batch_size)
            states, actions, rewards, next_states = extract_tensors(experiences)
            current_q_values = QV.get_current(policy_net, states, actions)
            next_q_values = QV.get_next(target_net, next_states)
            target_q_values = (next_q_values * gamma) + rewards

            loss = loss_function(current_q_values, target_q_values.unsqueeze(1))
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        if step >= weight_first_inc and step % weight_increment == 0:
            weight_update += 1
        if em.done:
            episode_durations.append(timestep)
            plot(episode_durations, 100, "output/plots/output.png")
            break
        if step % target_update == 0:
            target_net.load_state_dict(policy_net.state_dict())
    #if episode > 250:                                    
    #    scheduler.step(episode_durations[-1])
    scheduler.step()

em.close()

        
